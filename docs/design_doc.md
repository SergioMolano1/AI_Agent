# ğŸ“‹ Documento de DiseÃ±o TÃ©cnico: Agente de DetecciÃ³n de Incidencias
## AI Engineer Test - Agent Factory

---

## 1. Entendimiento del Problema (Â¿QuÃ© estamos resolviendo?)

### 1.1 La situaciÃ³n actual (El "dolor")

Imagina que trabajas en una empresa que procesa pagos digitales. Cada dÃ­a llegan **cientos de archivos** de diferentes proveedores: transacciones, liquidaciones, devoluciones, reportes regulatorios. Estos archivos llegan a **18 fuentes de datos diferentes** (piensa en ellas como 18 buzones, cada uno con sus propias reglas).

Hoy, un equipo de personas revisa **manualmente** estos 18 buzones cada maÃ±ana durante **3-4 horas** buscando problemas como:

- "Â¿LlegÃ³ el archivo que esperÃ¡bamos?" (Missing files)
- "Â¿LlegÃ³ duplicado?" (Duplicated files)
- "Â¿Por quÃ© estÃ¡ vacÃ­o si normalmente tiene datos?" (Empty files)
- "Â¿Por quÃ© tiene 100 registros si normalmente tiene 100,000?" (Volume anomalies)
- "Â¿Por quÃ© llegÃ³ a las 3am si siempre llega a las 8am?" (Late uploads)
- "Â¿Este archivo es de ayer o de hace una semana?" (Previous period files)

### 1.2 Lo que queremos construir (La soluciÃ³n)

Un **agente de IA** que haga este trabajo automÃ¡ticamente y genere un **reporte diario** que diga:

> ğŸ”´ "Â¡URGENTE! En la fuente X faltan 14 archivos. Necesitan acciÃ³n inmediata."
> ğŸŸ¡ "En la fuente Y el volumen es 30% mayor de lo normal. Investigar."
> ğŸŸ¢ "Las fuentes Z, W, V estÃ¡n perfectas."

### 1.3 El valor de negocio

| Impacto | Sin agente | Con agente |
|---------|-----------|------------|
| **Tiempo** | 3-4 horas/dÃ­a manuales | ~2 minutos automatizados |
| **Financiero** | Multas $10K-$50K USD por errores | DetecciÃ³n temprana |
| **Operacional** | 20+ horas/semana | Equipo enfocado en resolver, no en buscar |
| **Reputacional** | Errores llegan a stakeholders | Reportes proactivos |

---

## 2. Los Datos (Â¿Con quÃ© contamos?)

### 2.1 Las Hojas de Vida (CVs) â€” "El manual de cada buzÃ³n"

Cada una de las 18 fuentes tiene un archivo markdown que describe su **comportamiento normal**. Esto es fundamental porque cada fuente es diferente:

| Fuente | Archivos/dÃ­a | Horario esperado (UTC) | Â¿Fines de semana? | % VacÃ­os normales |
|--------|-------------|----------------------|-------------------|-------------------|
| 195385 - Settlement_Layout_2 | 32-45 (Mar-SÃ¡b) | 08:00-08:13 | SÃ¡b sÃ­, Dom no | 0% |
| 195436 - MyPal_DBR RX | 1 | 14:00-15:00 | SÃ­, ambos | 30% (Lun/Mar) |
| 195439 - MyPal_Activity report | 1 | 02:00 | SÃ­, ambos | 0% |
| 196125 - Settlement_Layout_1 | 41-53 (Mar-SÃ¡b) | 08:00-08:12 | SÃ¡b sÃ­, Dom no | 2.6% |
| 199944 - Soop Transaction PIX 3 | 2 (PIX+BANKING) | 11:01-11:33 | SÃ­, ambos | 5.4% |
| 207936 - Soop Tipo 2 | 3 (SHOP/PAGO/POS) | 11:01-11:44 | SÃ­, ambos | 33% (POS) |
| 207938 - Soop Tipo 3 | 3 (similar a Tipo2) | 10:45-12:30 | Lun-SÃ¡b, Dom no | 31.9% (POS) |
| 209773 - Desco PIX | 1 | 15:10-15:25 | SÃ­, ambos | 7% |
| 211544 - Desco DevoluÃ§Ãµes | 1 | 15:00-16:30 | SÃ­, ambos | 0% |
| 220504 - Payments_Layout_1_V3 | 16-19 | 08:08-08:18 | SÃ­, ambos | 24.8% (normal para ciertas entidades) |
| 220505 - Payments_Layout_2_V3 | 2 (Debito+MVP) | 08:02-08:11 | SÃ­, ambos | 3.7% (filtered jobs) |
| 220506 - Payments_Layout_3_V3 | 1 (_BR_3DS) | 08:03-08:19 | SÃ­, ambos | 0% |
| 224602 - Itm Pagamentos | 1 | 14:00-15:00 | SÃ­, ambos | 0% (post-backfill) |
| 224603 - Itm DevoluÃ§Ã£o | 1 | 12:15 (actual) | SÃ­, ambos | 0% (post-ene2025) |
| 228036 - WuPay_Sale payments_2 | 2 (Lun-Vie) | 20:10-20:25 | SÃ¡b esporÃ¡dico, Dom no | 6.5% |
| 228038 - WuPay_STL payments_2 | 2 (Lun-Vie) | 20:05-20:35 | SÃ¡b esporÃ¡dico, Dom no | 46.5% (alto pero normal) |
| 239611 - WuPay_Sale_adjustments_3 | 2 (Lun-Vie) | 20:12-20:23 | SÃ¡b esporÃ¡dico, Dom no | 9% |
| 239613 - WuPay_STL adjustments_3 | 2 (Lun-Vie) | 20:05-20:23 | SÃ¡b esporÃ¡dico, Dom no | 9.9% |

**Punto clave**: No puedes aplicar las mismas reglas a todas las fuentes. Un archivo vacÃ­o en `MyPal_DBR RX` un lunes es **normal** (80%+ son vacÃ­os). El mismo archivo vacÃ­o en `Desco DevoluÃ§Ãµes` serÃ­a una **incidencia grave** (0% histÃ³rico de vacÃ­os).

### 2.2 Los Datos Diarios â€” "Lo que llegÃ³ hoy"

Para cada dÃ­a (2025-09-08 al 2025-09-12) tenemos:

- **`files.json`**: Los Ãºltimos 200 archivos de cada fuente (incluye archivos de hoy y recientes). Cada archivo tiene:
  - `filename`: Nombre del archivo
  - `rows`: Cantidad de registros
  - `status`: processed / empty / failure / stopped / deleted
  - `is_duplicated`: true/false
  - `uploaded_at`: Timestamp de carga
  - `file_size`: TamaÃ±o en MB

- **`files_last_weekday.json`**: Archivos del mismo dÃ­a de la semana anterior (para comparaciÃ³n directa). Por ejemplo, si hoy es martes, contiene los archivos del martes pasado.

### 2.3 El Feedback â€” "QuÃ© opinÃ³ el cliente de reportes anteriores"

Un archivo Excel con 3 dÃ­as de feedback (sept 8, 9, 10) que contiene:
- El reporte generado por una versiÃ³n alpha
- Comentarios del stakeholder sobre quÃ© mejorar
- MÃ©tricas de accuracy (ej: 90%)

**Hallazgos clave del feedback:**
1. âœ… Ser mÃ¡s directo: "Faltan 3 archivos de X, Y, Z" en vez de texto tÃ©cnico largo
2. âœ… Eliminar jerga: No decir "re-trigger ingestion" ni "check landing location"
3. âœ… Combinar resumen + detalle: "Solo 4/18 archivos recibidos" + lista de faltantes
4. âœ… Las acciones recomendadas deben ser entendibles por el cliente
5. âœ… La secciÃ³n amarilla debe explicar claramente el problema

---

## 3. Arquitectura del Agente (Â¿CÃ³mo lo construimos?)

### 3.1 Â¿Por quÃ© Google ADK?

**ADK (Agent Development Kit)** es el framework de Google para construir agentes de IA. Lo elegimos porque:

1. **OrquestaciÃ³n multi-agente nativa**: Podemos crear sub-agentes especializados (uno por cada tipo de incidencia) y un agente principal que los coordina
2. **IntegraciÃ³n con Gemini**: Usa modelos de Google (Gemini) como LLM base
3. **Tools nativas**: Permite definir funciones Python como herramientas que el agente puede invocar
4. **Callbacks y sesiones**: Control granular del flujo de ejecuciÃ³n
5. **Ecosistema GCP**: Se integra naturalmente con Vertex AI, Cloud Storage, BigQuery, etc.

### 3.2 Diagrama de Arquitectura (Vista General)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ORQUESTADOR PRINCIPAL                        â”‚
â”‚                 (IncidentDetectionAgent)                        â”‚
â”‚              Modelo: gemini-2.0-flash                           â”‚
â”‚                                                                 â”‚
â”‚  Responsabilidad: Coordinar sub-agentes, consolidar reporte    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PREPARADOR   â”‚â”€â”€â”€â–¶â”‚         SUB-AGENTES DETECTORES       â”‚  â”‚
â”‚  â”‚  DE INPUTS    â”‚    â”‚                                      â”‚  â”‚
â”‚  â”‚              â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚ â€¢ parse_cv() â”‚    â”‚  â”‚Missing â”‚ â”‚Duplicatâ”‚ â”‚ Empty  â”‚   â”‚  â”‚
â”‚  â”‚ â€¢ parse_     â”‚    â”‚  â”‚ File   â”‚ â”‚& Failedâ”‚ â”‚ File   â”‚   â”‚  â”‚
â”‚  â”‚   files()    â”‚    â”‚  â”‚Detectorâ”‚ â”‚Detectorâ”‚ â”‚Detectorâ”‚   â”‚  â”‚
â”‚  â”‚ â€¢ filter_    â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚   today()    â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚Volume  â”‚ â”‚Late    â”‚ â”‚Previousâ”‚   â”‚  â”‚
â”‚                      â”‚  â”‚Variati.â”‚ â”‚Upload  â”‚ â”‚Period  â”‚   â”‚  â”‚
â”‚                      â”‚  â”‚Detectorâ”‚ â”‚Detectorâ”‚ â”‚Detectorâ”‚   â”‚  â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚                          â”‚
â”‚                                     â–¼                          â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                      â”‚      CONSOLIDADOR DE REPORTE          â”‚  â”‚
â”‚                      â”‚                                      â”‚  â”‚
â”‚                      â”‚  â€¢ Agrupa incidencias por fuente     â”‚  â”‚
â”‚                      â”‚  â€¢ Clasifica severidad (ğŸ”´ğŸŸ¡ğŸŸ¢)       â”‚  â”‚
â”‚                      â”‚  â€¢ Genera recomendaciones claras     â”‚  â”‚
â”‚                      â”‚  â€¢ Formato ejecutivo para negocio    â”‚  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Componentes en Detalle

#### A) Preparador de Inputs (Tools/Functions â€” No es un agente LLM)

Este componente **NO usa LLM**. Son funciones Python puras que procesan datos:

```
Â¿Por quÃ© no usar LLM aquÃ­?
â†’ Parsear JSON y markdown es determinÃ­stico
â†’ No necesitas "inteligencia" para filtrar archivos por fecha
â†’ Es mÃ¡s rÃ¡pido, barato y predecible
â†’ En ADK, estas se definen como "Tools" (FunctionTool)
```

**Funciones principales:**

| FunciÃ³n | Input | Output | DescripciÃ³n |
|---------|-------|--------|-------------|
| `parse_cv(source_id)` | Archivo .md del CV | Objeto estructurado con patrones | Extrae horarios, volÃºmenes esperados, % vacÃ­os normales por dÃ­a |
| `load_today_files(source_id, date)` | files.json + fecha | Lista de archivos de hoy | Filtra files.json por `uploaded_at` del dÃ­a de ejecuciÃ³n |
| `load_last_weekday_files(source_id)` | files_last_weekday.json | Lista de archivos | Carga archivos del mismo dÃ­a semana anterior |
| `get_cv_summary(source_id)` | CV parseado | Texto resumido | Resume el CV en formato que el LLM pueda consumir eficientemente |

#### B) Detectores de Incidencias (Sub-Agentes ADK)

Cada detector es un **sub-agente** con su propia instrucciÃ³n (prompt) especializada. Esto es clave en ADK porque permite:

- **SeparaciÃ³n de responsabilidades**: Cada detector tiene un prompt enfocado
- **ReutilizaciÃ³n**: Puedes mejorar un detector sin tocar los demÃ¡s
- **Testeo independiente**: Puedes evaluar cada detector por separado

**Detector 1: Missing File Detector**
```
PropÃ³sito: Â¿LlegÃ³ todo lo que debÃ­a llegar?

LÃ³gica:
1. Del CV â†’ obtener cuÃ¡ntos archivos se esperan para este dÃ­a de la semana
2. De files.json â†’ contar cuÃ¡ntos archivos llegaron hoy
3. De files_last_weekday.json â†’ cuÃ¡ntos llegaron el mismo dÃ­a la semana pasada
4. Si llegaron menos de lo esperado â†’ INCIDENCIA

Ejemplo:
  CV dice: "Martes = 40 archivos (Â±11)"
  Hoy (martes) llegaron: 30 archivos
  Martes pasado: 39 archivos
  â†’ INCIDENCIA: Faltan ~10 archivos vs. lo esperado

Severidad: URGENTE si faltan archivos crÃ­ticos
```

**Detector 2: Duplicated & Failed File Detector**
```
PropÃ³sito: Â¿Hay archivos duplicados o con errores?

LÃ³gica:
1. Filtrar archivos de hoy donde is_duplicated == TRUE
2. Filtrar archivos de hoy donde status == "stopped" o "failure"
3. Verificar si hay nombres de archivo repetidos
4. Reportar cada caso encontrado

Severidad: URGENTE si duplicados + stopped/failure
```

**Detector 3: Unexpected Empty File Detector**
```
PropÃ³sito: Â¿Hay archivos vacÃ­os que no deberÃ­an estarlo?

LÃ³gica:
1. Filtrar archivos de hoy con rows == 0
2. Del CV â†’ verificar si ESA fuente normalmente tiene vacÃ­os ese dÃ­a
3. Si el CV dice "Lunes: 80% vacÃ­os" y hoy es lunes â†’ NO es incidencia
4. Si el CV dice "0% vacÃ­os" y hay un vacÃ­o â†’ SÃ es incidencia

Caso especial: Fuentes como 207936 (Soop Tipo 2) tienen POS siempre vacÃ­o = NORMAL
Caso especial: 220504 tiene Innovation/POC/safemode siempre vacÃ­os = NORMAL

Severidad: REQUIERE ATENCIÃ“N (a menos que sea patrÃ³n conocido)
```

**Detector 4: Unexpected Volume Variation Detector**
```
PropÃ³sito: Â¿El volumen de registros es anormal?

LÃ³gica:
1. Del CV â†’ obtener mean, median, stdev de rows para este dÃ­a de la semana
2. De files.json â†’ obtener rows de los archivos de hoy
3. Calcular si estÃ¡ fuera del rango esperado (mean Â± 2*stdev o intervalo 95%)
4. Comparar tambiÃ©n con files_last_weekday

Ejemplo:
  CV dice: "MiÃ©rcoles mean=131,025, stdev=~50,000"
  Hoy (miÃ©rcoles): 5,000 registros
  â†’ INCIDENCIA: Volumen 96% menor al esperado

IMPORTANTE: Comparar por dÃ­a de la semana, no globalmente.
Verificar si fines de semana aplica distinto comportamiento.

Severidad: URGENTE si la desviaciÃ³n es extrema (>3 stdev)
           REQUIERE ATENCIÃ“N si moderada (>2 stdev)
```

**Detector 5: File Upload After Schedule Detector**
```
PropÃ³sito: Â¿Llegaron archivos muy tarde?

LÃ³gica:
1. Del CV â†’ obtener ventana de upload esperada para este dÃ­a
2. De files.json â†’ obtener uploaded_at de archivos de hoy
3. Si uploaded_at > ventana_esperada + 4 horas â†’ INCIDENCIA

Ejemplo:
  CV dice: "Upload esperado: 08:00-08:18 UTC"
  Archivo llegÃ³: 14:30 UTC (+6 horas)
  â†’ INCIDENCIA: Archivo 6 horas tarde

REGLA: Este incidente es SIEMPRE tipo "advertencia" (ğŸŸ¡)
       NUNCA debe ser clasificado como urgente (ğŸ”´)

Severidad: REQUIERE ATENCIÃ“N (mÃ¡ximo)
```

**Detector 6: Upload of Previous File Detector**
```
PropÃ³sito: Â¿Llegaron archivos de periodos anteriores?

LÃ³gica:
1. Del CV â†’ obtener el ECD (Expected Coverage Data) â€” quÃ© fechas cubre normalmente
2. De files.json â†’ verificar si el archivo tiene fecha de nombre fuera del ECD
3. Estos suelen ser subidas manuales/histÃ³ricas

Ejemplo:
  Hoy es 2025-09-10
  Llega archivo con fecha 2025-09-05 en el nombre
  Lag habitual del CV = 1 dÃ­a
  â†’ INCIDENCIA: Archivo de periodo anterior (probablemente backfill manual)

REGLA: NUNCA clasificar como error crÃ­tico
       Es informativo, indica subida manual/histÃ³rica

Severidad: REQUIERE ATENCIÃ“N (mÃ¡ximo)
```

#### C) Consolidador de Reporte (Agente LLM)

Este sÃ­ usa el LLM porque necesita **sintetizar**, **priorizar** y **redactar** en lenguaje de negocio:

```
Input: Resultados de los 6 detectores para las 18 fuentes
Output: Reporte ejecutivo con:

ğŸ”´ URGENTE - AcciÃ³n Inmediata Requerida
   Criterio: >1 archivo con incidente urgente O >3 incidentes requiere atenciÃ³n
   
ğŸŸ¡ REQUIERE ATENCIÃ“N - Necesita InvestigaciÃ³n  
   Criterio: Al menos 1 incidente que requiera atenciÃ³n

ğŸŸ¢ TODO BIEN - Sin Problemas
   Criterio: No hay incidentes

+ Recomendaciones claras en lenguaje de negocio (sin jerga tÃ©cnica)
```

### 3.4 PatrÃ³n ADK: Â¿CÃ³mo se conectan los componentes?

En Google ADK, la estructura se implementa asÃ­:

```
Orquestador (Agent principal)
â”œâ”€â”€ Tools (FunctionTool):
â”‚   â”œâ”€â”€ parse_cv()
â”‚   â”œâ”€â”€ load_today_files()
â”‚   â”œâ”€â”€ load_last_weekday_files()
â”‚   â””â”€â”€ get_source_list()
â”‚
â”œâ”€â”€ Sub-Agents (cada uno es un Agent ADK):
â”‚   â”œâ”€â”€ MissingFileDetector (Agent con instrucciones especializadas + tools)
â”‚   â”œâ”€â”€ DuplicatedFailedDetector (Agent con instrucciones especializadas + tools)
â”‚   â”œâ”€â”€ EmptyFileDetector (Agent con instrucciones especializadas + tools)
â”‚   â”œâ”€â”€ VolumeVariationDetector (Agent con instrucciones especializadas + tools)
â”‚   â”œâ”€â”€ LateUploadDetector (Agent con instrucciones especializadas + tools)
â”‚   â””â”€â”€ PreviousPeriodDetector (Agent con instrucciones especializadas + tools)
â”‚
â””â”€â”€ Sub-Agent:
    â””â”€â”€ ReportConsolidator (Agent que genera el reporte final)
```

**Flujo de ejecuciÃ³n simplificado:**

```
1. Usuario ejecuta el agente con la fecha del dÃ­a
   â”‚
2. Orquestador invoca Tools para preparar datos
   â”‚  â†’ parse_cv() para cada fuente
   â”‚  â†’ load_today_files() para cada fuente  
   â”‚  â†’ load_last_weekday_files() para cada fuente
   â”‚
3. Orquestador delega a cada Sub-Agente Detector
   â”‚  â†’ Cada detector recibe: CV parseado + archivos de hoy + archivos semana pasada
   â”‚  â†’ Cada detector devuelve: lista de incidencias encontradas (o vacÃ­a)
   â”‚
4. Orquestador recopila todos los resultados
   â”‚
5. Orquestador delega al ReportConsolidator
   â”‚  â†’ Recibe: todas las incidencias de todos los detectores
   â”‚  â†’ Genera: reporte ejecutivo con severidad y recomendaciones
   â”‚
6. Reporte final entregado
```

---

## 4. Estrategia de EvaluaciÃ³n (Â¿CÃ³mo sabemos que funciona bien?)

### 4.1 Â¿Por quÃ© es importante evaluar?

No basta con que el agente "funcione". Necesitamos **medir quÃ© tan bien funciona** y **mejorar iterativamente**. Esto es lo que diferencia a un prototipo de un sistema de producciÃ³n.

### 4.2 Ground Truth: El Feedback como Referencia

El feedback del stakeholder nos da **3 dÃ­as de "respuesta correcta"** (sept 8, 9, 10). De ahÃ­ extraemos:

| Fecha | Incidencias reales reportadas | Accuracy del alpha |
|-------|------------------------------|-------------------|
| Sept 8 | 3 urgentes (220504, 220505, 220506 missing files) + 4 atenciÃ³n | No reportada |
| Sept 9 | 4 urgentes (196125, 207936, 207938, 199944) + 6 atenciÃ³n | No reportada |
| Sept 10 | 5 urgentes (220504, 220505, 220506, 196125, 195385) | 90% |

### 4.3 MÃ©tricas de EvaluaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MÃ‰TRICAS DEL AGENTE                       â”‚
â”‚                                                             â”‚
â”‚  1. DETECCIÃ“N (Â¿Encuentra los problemas?)                   â”‚
â”‚     â”œâ”€â”€ Precision: De lo que reportÃ³, Â¿cuÃ¡nto era real?     â”‚
â”‚     â”œâ”€â”€ Recall: De lo real, Â¿cuÃ¡nto detectÃ³?                â”‚
â”‚     â””â”€â”€ F1-Score: Balance entre ambos                       â”‚
â”‚                                                             â”‚
â”‚  2. CLASIFICACIÃ“N (Â¿Los clasifica bien?)                    â”‚
â”‚     â”œâ”€â”€ Accuracy de severidad: ğŸ”´ğŸŸ¡ğŸŸ¢ correctos            â”‚
â”‚     â””â”€â”€ Confusion matrix: Â¿Confunde urgente con atenciÃ³n?   â”‚
â”‚                                                             â”‚
â”‚  3. CALIDAD DEL REPORTE (Â¿Es Ãºtil para el negocio?)        â”‚
â”‚     â”œâ”€â”€ Claridad: Â¿Lenguaje entendible? (eval LLM-as-judge)â”‚
â”‚     â”œâ”€â”€ Accionabilidad: Â¿Las recomendaciones son Ãºtiles?    â”‚
â”‚     â””â”€â”€ Completitud: Â¿Incluye toda la info necesaria?       â”‚
â”‚                                                             â”‚
â”‚  4. RENDIMIENTO                                             â”‚
â”‚     â”œâ”€â”€ Tiempo de ejecuciÃ³n                                 â”‚
â”‚     â”œâ”€â”€ Tokens consumidos                                   â”‚
â”‚     â””â”€â”€ Costo por ejecuciÃ³n                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.4 Pipeline de EvaluaciÃ³n

```
Para cada dÃ­a de prueba (sept 8-12):
â”‚
â”œâ”€â”€ 1. Ejecutar agente â†’ obtener reporte generado
â”‚
â”œâ”€â”€ 2. Comparar vs. ground truth (feedback)
â”‚   â”œâ”€â”€ Â¿DetectÃ³ las mismas incidencias? (Precision/Recall)
â”‚   â”œâ”€â”€ Â¿AsignÃ³ la misma severidad? (Accuracy clasificaciÃ³n)
â”‚   â””â”€â”€ Â¿Las recomendaciones son mejores que el alpha? (LLM-as-judge)
â”‚
â”œâ”€â”€ 3. Calcular mÃ©tricas agregadas
â”‚   â”œâ”€â”€ Precision promedio
â”‚   â”œâ”€â”€ Recall promedio
â”‚   â”œâ”€â”€ F1-Score
â”‚   â””â”€â”€ Accuracy de severidad
â”‚
â””â”€â”€ 4. Registrar resultados para comparar entre versiones
```

### 4.5 EvaluaciÃ³n con LLM-as-Judge (TÃ©cnica avanzada)

Para evaluar la **calidad del texto** del reporte (no solo si detectÃ³ bien), usamos otro LLM como juez:

```
Prompt al LLM evaluador:
"Dado este feedback del stakeholder: [feedback]
Y este reporte generado por el agente: [reporte]

EvalÃºa del 1-10:
1. Â¿El lenguaje es claro y sin jerga tÃ©cnica?
2. Â¿Las recomendaciones son accionables para un no-tÃ©cnico?
3. Â¿El resumen es directo y conciso?
4. Â¿IdentificÃ³ correctamente los problemas del feedback?"
```

---

## 5. Estrategia de Versionamiento Evolutivo

### 5.1 El concepto: Build â†’ Evaluate â†’ Improve â†’ Repeat

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  v1.0    â”‚â”€â”€â”€â”€â–¶â”‚ EVALUAR  â”‚â”€â”€â”€â”€â–¶â”‚  v2.0    â”‚â”€â”€â”€â”€â–¶ ...
  â”‚ Baseline â”‚     â”‚ mÃ©tricas â”‚     â”‚ Mejorado â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ feedback â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Plan de Versiones

#### v1.0 â€” Baseline (Reglas + LLM bÃ¡sico)
```
QuÃ© hace:
- Detectores basados principalmente en reglas determinÃ­sticas
- LLM solo para consolidar y redactar el reporte
- Prompts simples y directos

QuÃ© evaluamos:
- Â¿Detecta las incidencias del feedback? (Recall)
- Â¿Genera falsos positivos? (Precision)
- Â¿El reporte es entendible?

Resultado esperado: Funcional pero con margen de mejora en el lenguaje
```

#### v2.0 â€” IncorporaciÃ³n del Feedback
```
Cambios basados en evaluaciÃ³n de v1.0:
- Mejorar prompts del consolidador con ejemplos del feedback
  ("Decir 'Faltan 14/18 archivos' en vez de listar tÃ©cnicamente")
- Ajustar umbrales de los detectores si v1 tuvo falsos positivos/negativos
- Agregar few-shot examples al prompt del consolidador usando reportes
  que el stakeholder aprobÃ³

QuÃ© evaluamos:
- Â¿MejorÃ³ el F1-Score vs v1?
- Â¿El lenguaje es mÃ¡s claro? (LLM-as-judge)
- Â¿Se eliminaron los problemas del feedback?
```

#### v3.0 â€” OptimizaciÃ³n de Arquitectura (Hybrid Approach)
```
Cambios basados en limitaciones reales de v1/v2:
- PROBLEMA: v1/v2 hacen ~20+ llamadas LLM â†’ rate limit en Gemini free tier (15 req/min)
- INSIGHT: La detecciÃ³n es DETERMINÃSTICA (contar archivos, comparar volÃºmenes = math, no lenguaje)
- SOLUCIÃ“N: Python puro para detecciÃ³n (6 detectores, 0 LLM calls) + 1 LLM call para reporte
- RESULTADO: De 20+ calls a 1-2 calls. Funciona en free tier. MÃ¡s rÃ¡pido y barato.

ADK Components en v3:
- Agent: Consolidador de reporte (hereda prompts mejorados de v2)
- FunctionTool: Envuelve el pipeline de detecciÃ³n Python
- Runner + InMemorySessionService: EjecuciÃ³n del agente

QuÃ© evaluamos:
- Â¿Misma calidad de detecciÃ³n que v1/v2? (precision/recall)
- Â¿Mejor rendimiento? (tiempo, costo, tasa de Ã©xito)
- Â¿El reporte mantiene calidad de lenguaje de v2?
```

#### Futuras mejoras
```
Posibles mejoras:
- Ajuste fino de umbrales por fuente
- DetecciÃ³n de patrones estacionales (ej: feriados de Curitiba)
- IntegraciÃ³n con MCP para enviar reportes a Slack/Email
- Uso de Vertex AI para analytics avanzados
```

### 5.3 CÃ³mo se evidencia el versionamiento (Git)

```
repo/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ v1/                          â† Multi-agente baseline
â”‚   â”‚   â”œâ”€â”€ agent.py                 â† Orquestador + 6 sub-agentes + consolidador
â”‚   â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â”œâ”€â”€ prompts/                 â† Prompts bÃ¡sicos
â”‚   â”‚   â””â”€â”€ tools/                   â† data_tools.py (compartido)
â”‚   â”œâ”€â”€ v2/                          â† Multi-agente con prompts mejorados
â”‚   â”‚   â”œâ”€â”€ agent.py                 â† Misma arquitectura
â”‚   â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â””â”€â”€ prompts/                 â† Prompts mejorados con feedback
â”‚   â””â”€â”€ v3/                          â† HÃ­brida (optimizaciÃ³n de arquitectura)
â”‚       â”œâ”€â”€ agent.py                 â† 1 agente ADK (solo reporte)
â”‚       â”œâ”€â”€ detectors/
â”‚       â”‚   â””â”€â”€ rule_based.py        â† 6 detectores Python puros
â”‚       â””â”€â”€ prompts/                 â† Prompt consolidador hereda de v2
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval_pipeline.py             â† Script de evaluaciÃ³n
â”‚   â”œâ”€â”€ ground_truth/                â† Feedback parseado como verdad
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ v1_results.json          â† MÃ©tricas v1
â”‚   â”‚   â””â”€â”€ v2_results.json          â† MÃ©tricas v2 (se comparan)
â”‚   â””â”€â”€ comparison_report.md         â† "v2 mejorÃ³ recall de 85% a 95%"
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ datasource_cvs/              â† Los 18 CVs
â”‚   â”œâ”€â”€ 2025-09-08_20_00_UTC/        â† Datos por dÃ­a
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ feedback/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md              â† Este documento
â”‚   â””â”€â”€ design_decisions.md
â”‚
â””â”€â”€ README.md
```

---

## 6. IntegraciÃ³n con Ecosistema GCP

### 6.1 Componentes GCP relevantes

| Componente GCP | Uso en el proyecto | Por quÃ© |
|---------------|-------------------|---------|
| **Vertex AI** | Hosting del modelo Gemini | Endpoint gestionado, escalable |
| **Cloud Storage (GCS)** | Almacenar CVs, archivos JSON, reportes | Fuente de datos centralizada |
| **BigQuery** | Analytics de mÃ©tricas de evaluaciÃ³n | Consultas SQL sobre resultados histÃ³ricos |
| **Cloud Functions** | Trigger diario del agente | EjecuciÃ³n serverless programada |
| **Cloud Scheduler** | Cron job diario | Dispara la ejecuciÃ³n a la hora correcta |
| **Secret Manager** | API keys, credenciales | Seguridad de secrets |
| **Artifact Registry** | Contenedor del agente | Despliegue versionado |

### 6.2 Flujo de producciÃ³n (visiÃ³n futura)

```
Cloud Scheduler (6:00 AM UTC)
  â”‚
  â–¼
Cloud Function (trigger)
  â”‚
  â–¼
Agente ADK (Vertex AI)
  â”œâ”€â”€ Lee CVs de GCS
  â”œâ”€â”€ Lee files.json de GCS
  â”œâ”€â”€ Ejecuta detectores
  â”œâ”€â”€ Genera reporte
  â”‚
  â–¼
Reporte â†’ GCS + Slack/Email (vÃ­a MCP Tool)
MÃ©tricas â†’ BigQuery (para dashboard de evaluaciÃ³n)
```

---

## 7. Criterios Opcionales (Bonus)

### 7.1 MCP Tool para mensajerÃ­a

Implementar un **MCP (Model Context Protocol) Server** custom o usar uno existente para enviar el reporte a Slack o Email:

```
OpciÃ³n A: MCP Server custom para Slack webhook
OpciÃ³n B: MCP Server para SendGrid (email)
OpciÃ³n C: Usar un MCP existente de la comunidad
```

### 7.2 TÃ©cnicas avanzadas

- **Few-shot prompting**: Incluir ejemplos de reportes buenos/malos en los prompts
- **Chain-of-Thought**: Hacer que los detectores "razonen" paso a paso
- **LLM-as-Judge**: Para evaluaciÃ³n automÃ¡tica de calidad del reporte
- **Structured Output**: Forzar JSON schema en las respuestas de los detectores

### 7.3 Patrones y buenas prÃ¡cticas

- **Separation of Concerns**: Cada detector es independiente
- **Dependency Injection**: Los datos se inyectan como tools, no hardcodeados
- **Idempotencia**: Ejecutar el agente 2 veces con los mismos datos da el mismo resultado
- **Logging estructurado**: Cada decisiÃ³n del agente se registra para debugging
- **Error handling**: Si un detector falla, los demÃ¡s siguen funcionando

---
